{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Landscape Image Classifier - Part 1: Setup\n",
    "\n",
    "## 📦 Imports & Configuration\n",
    "\n",
    "This cell handles:\n",
    "- Essential library imports\n",
    "- GPU configuration for accelerated training\n",
    "- Random seed setup for reproducibility\n",
    "\n",
    "### Key Components:\n",
    "- **TensorFlow/Keras** - Deep learning framework\n",
    "- **Matplotlib/Seaborn** - Visualization\n",
    "- **Scikit-learn** - Evaluation metrics\n",
    "- **GPU Configuration** - Automatic GPU detection and memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.utils import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import subprocess\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Configure GPU settings\n",
    "def configure_gpu(memory_limit=4096):\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.set_visible_devices(gpus[0], \"GPU\")\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpus[0],\n",
    "                [\n",
    "                    tf.config.experimental.VirtualDeviceConfiguration(\n",
    "                        memory_limit=memory_limit\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "            print(f\"Configured NVIDIA GPU (GPU 0) with {memory_limit}MB limit\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU configuration error: {e}\")\n",
    "    else:\n",
    "        print(\"No GPU detected. Falling back to CPU.\")\n",
    "\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    IMG_HEIGHT = 160\n",
    "    IMG_WIDTH = 160\n",
    "    IMG_CHANNELS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 35\n",
    "    LEARNING_RATE = 1e-4\n",
    "    TRAIN_DIR = \"image_dataset/seg_train/seg_train\"\n",
    "    TEST_DIR = \"image_dataset/seg_test/seg_test\"\n",
    "    MODEL_SAVE_DIR = \"saved_models\"\n",
    "    CHECKPOINT_PATH = \"saved_models/best_model.keras\"\n",
    "    TFLITE_PATH = \"tflite/model.tflite\"\n",
    "    TFJS_MODEL_DIR = \"tfjs_model\"\n",
    "    CLASSES = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    MODEL_JSON_PATH = \"tfjs_model/model_info.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌄 Landscape Image Classifier - Part 2: Core Class\n",
    "\n",
    "## 🔧 `LandscapeClassifier` Class\n",
    "\n",
    "This is the main class that handles:\n",
    "- Model architecture (EfficientNetB0 base)\n",
    "- Training pipeline\n",
    "- Evaluation metrics\n",
    "- Model conversion (TFLite, TFJS, SavedModel)\n",
    "\n",
    "### Key Features:\n",
    "- **Transfer Learning**: Uses pre-trained EfficientNetB0\n",
    "- **Data Augmentation**: Automatic image preprocessing\n",
    "- **Multi-format Export**: Saves models in 3 formats\n",
    "- **Error Recovery**: Automatic fallback mechanisms\n",
    "\n",
    "### Architecture:\n",
    "1. **Base Model**: EfficientNetB0 (frozen weights)\n",
    "2. **Custom Head**:\n",
    "   - GlobalAveragePooling2D\n",
    "   - Dense(256, ReLU)\n",
    "   - Dropout(0.5)\n",
    "   - Softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LandscapeClassifier:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "\n",
    "        # Create necessary directories\n",
    "        os.makedirs(self.config.MODEL_SAVE_DIR, exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(self.config.TFLITE_PATH), exist_ok=True)\n",
    "        os.makedirs(self.config.TFJS_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "        # Create initial model.json to confirm program execution\n",
    "        self._create_initial_model_json()\n",
    "        self._create_initial_tfjs_model_json()\n",
    "\n",
    "    # Paste this exact function here\n",
    "    def build_model(self):\n",
    "        base_model = EfficientNetB0(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False,\n",
    "            input_shape=(\n",
    "                self.config.IMG_HEIGHT,\n",
    "                self.config.IMG_WIDTH,\n",
    "                self.config.IMG_CHANNELS,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Unfreeze more layers for fine-tuning\n",
    "        base_model.trainable = True\n",
    "        for layer in base_model.layers[:-50]:  # Unfreeze the top 50 layers\n",
    "            layer.trainable = False\n",
    "\n",
    "        self.model = models.Sequential(\n",
    "            [\n",
    "                base_model,\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "                layers.Dense(512),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Activation(\"relu\"),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(256),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Activation(\"relu\"),\n",
    "                layers.Dropout(0.3),\n",
    "                layers.Dense(self.config.NUM_CLASSES, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        self._update_model_json(\n",
    "            \"model_built\",\n",
    "            {\n",
    "                \"model\": {\n",
    "                    \"base_model\": \"EfficientNetB0\",\n",
    "                    \"trainable_base\": \"Partial (top 50 layers)\",\n",
    "                    \"total_parameters\": self.model.count_params(),\n",
    "                    \"compile_config\": {\n",
    "                        \"optimizer\": \"Adam\",\n",
    "                        \"learning_rate\": self.config.LEARNING_RATE,\n",
    "                        \"loss\": \"categorical_crossentropy\",\n",
    "                        \"metrics\": [\"accuracy\"],\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train(self):\n",
    "        self._create_data_pipeline()\n",
    "\n",
    "        # Update model.json to indicate training started\n",
    "        self._update_model_json(\"training_started\")\n",
    "\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                self.config.CHECKPOINT_PATH,\n",
    "                save_best_only=True,\n",
    "                monitor=\"val_accuracy\",\n",
    "                mode=\"max\",\n",
    "                verbose=1,\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_accuracy\",\n",
    "                patience=15,  # Increased from 10 to 15\n",
    "                restore_best_weights=True,\n",
    "                verbose=1,\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir=\"./saved_models/logs\"),  # Added for monitoring\n",
    "        ]\n",
    "\n",
    "        # Calculate class weights to handle imbalance\n",
    "        y_train = np.concatenate([y for _, y in self.train_dataset], axis=0).argmax(\n",
    "            axis=1\n",
    "        )\n",
    "        class_weights = compute_class_weight(\n",
    "            \"balanced\", classes=np.unique(y_train), y=y_train\n",
    "        )\n",
    "        class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            validation_data=self.validation_dataset,\n",
    "            epochs=self.config.EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,  # Added to balance classes\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Update model.json with training results\n",
    "        training_info = {\n",
    "            \"training\": {\n",
    "                \"completed\": True,\n",
    "                \"epochs_completed\": len(history.history[\"accuracy\"]),\n",
    "                \"best_val_accuracy\": float(max(history.history[\"val_accuracy\"])),\n",
    "                \"final_val_accuracy\": float(history.history[\"val_accuracy\"][-1]),\n",
    "                \"best_val_loss\": float(min(history.history[\"val_loss\"])),\n",
    "                \"final_val_loss\": float(history.history[\"val_loss\"][-1]),\n",
    "            }\n",
    "        }\n",
    "        self._update_model_json(\"training_completed\", training_info)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def _create_initial_model_json(self):\n",
    "        \"\"\"Create an initial model.json file at the start of program execution\"\"\"\n",
    "        model_info = {\n",
    "            \"status\": \"initialized\",\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"configuration\": {\n",
    "                \"img_dimensions\": f\"{self.config.IMG_HEIGHT}x{self.config.IMG_WIDTH}x{self.config.IMG_CHANNELS}\",\n",
    "                \"batch_size\": self.config.BATCH_SIZE,\n",
    "                \"epochs\": self.config.EPOCHS,\n",
    "                \"learning_rate\": self.config.LEARNING_RATE,\n",
    "                \"classes\": self.config.CLASSES,\n",
    "                \"num_classes\": self.config.NUM_CLASSES,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Save model info to JSON file\n",
    "        with open(self.config.MODEL_JSON_PATH, \"w\") as f:\n",
    "            json.dump(model_info, f, indent=4)\n",
    "\n",
    "        print(f\"Initial model.json created at {self.config.MODEL_JSON_PATH}\")\n",
    "\n",
    "    def _create_initial_tfjs_model_json(self):\n",
    "        \"\"\"Create a basic model.json file for TensorFlow.js at program start\"\"\"\n",
    "        tfjs_model_json_path = os.path.join(self.config.TFJS_MODEL_DIR, \"model.json\")\n",
    "\n",
    "        # Create a simplified model.json structure\n",
    "        model_json_content = {\n",
    "            \"format\": \"tfjs-layers-model\",\n",
    "            \"generatedBy\": \"LandscapeClassifier\",\n",
    "            \"convertedBy\": \"TensorFlow.js Converter\",\n",
    "            \"modelTopology\": {\n",
    "                \"class_name\": \"Sequential\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"sequential\",\n",
    "                    \"layers\": [\n",
    "                        {\n",
    "                            \"class_name\": \"EfficientNetB0\",\n",
    "                            \"config\": {\"trainable\": False},\n",
    "                        },\n",
    "                        {\"class_name\": \"GlobalAveragePooling2D\", \"config\": {}},\n",
    "                        {\n",
    "                            \"class_name\": \"Dense\",\n",
    "                            \"config\": {\"units\": 256, \"activation\": \"relu\"},\n",
    "                        },\n",
    "                        {\"class_name\": \"Dropout\", \"config\": {\"rate\": 0.5}},\n",
    "                        {\n",
    "                            \"class_name\": \"Dense\",\n",
    "                            \"config\": {\n",
    "                                \"units\": self.config.NUM_CLASSES,\n",
    "                                \"activation\": \"softmax\",\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"weightsManifest\": [],\n",
    "            \"signature\": {\n",
    "                \"inputs\": {\n",
    "                    \"input_1\": {\n",
    "                        \"name\": \"input_1\",\n",
    "                        \"dtype\": \"float32\",\n",
    "                        \"shape\": [\n",
    "                            None,\n",
    "                            self.config.IMG_HEIGHT,\n",
    "                            self.config.IMG_WIDTH,\n",
    "                            self.config.IMG_CHANNELS,\n",
    "                        ],\n",
    "                    }\n",
    "                },\n",
    "                \"outputs\": {\n",
    "                    \"dense_1\": {\n",
    "                        \"name\": \"dense_1\",\n",
    "                        \"dtype\": \"float32\",\n",
    "                        \"shape\": [None, self.config.NUM_CLASSES],\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Convert Python None to JSON null via the json module\n",
    "        with open(tfjs_model_json_path, \"w\") as f:\n",
    "            json.dump(model_json_content, f, indent=2)\n",
    "\n",
    "        print(f\"Initial TensorFlow.js model.json created at {tfjs_model_json_path}\")\n",
    "\n",
    "    def _update_model_json(self, status, additional_info=None):\n",
    "        \"\"\"Update the model.json file with current status and info\"\"\"\n",
    "        if os.path.exists(self.config.MODEL_JSON_PATH):\n",
    "            with open(self.config.MODEL_JSON_PATH, \"r\") as f:\n",
    "                model_info = json.load(f)\n",
    "        else:\n",
    "            model_info = {}\n",
    "\n",
    "        model_info[\"status\"] = status\n",
    "        model_info[\"last_updated\"] = datetime.datetime.now().strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "\n",
    "        if additional_info:\n",
    "            model_info.update(additional_info)\n",
    "\n",
    "        with open(self.config.MODEL_JSON_PATH, \"w\") as f:\n",
    "            json.dump(model_info, f, indent=4)\n",
    "\n",
    "        print(f\"Updated model.json with status: {status}\")\n",
    "\n",
    "    def _create_data_pipeline(self):\n",
    "        def parse_image(image, label):\n",
    "            image = tf.image.resize(\n",
    "                image, [self.config.IMG_HEIGHT, self.config.IMG_WIDTH]\n",
    "            )\n",
    "            image = image / 255.0\n",
    "            # Enhanced data augmentation (without random_rotation)\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_flip_up_down(image)\n",
    "            image = tf.image.random_brightness(image, 0.2)\n",
    "            image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "            image = tf.image.random_saturation(image, 0.8, 1.2)\n",
    "            image = tf.image.random_hue(image, 0.1)  # Still works for augmentation\n",
    "            return image, label\n",
    "\n",
    "        def get_dataset(directory, subset):\n",
    "            dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                directory,\n",
    "                image_size=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH),\n",
    "                batch_size=self.config.BATCH_SIZE,\n",
    "                label_mode=\"categorical\",\n",
    "                subset=subset,\n",
    "                validation_split=0.2,\n",
    "                seed=42,\n",
    "            )\n",
    "            return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        self.train_dataset = (\n",
    "            get_dataset(self.config.TRAIN_DIR, \"training\")\n",
    "            .cache()\n",
    "            .map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        )\n",
    "        self.validation_dataset = get_dataset(\n",
    "            self.config.TRAIN_DIR, \"validation\"\n",
    "        ).cache()\n",
    "        self.test_dataset = (\n",
    "            tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                self.config.TEST_DIR,\n",
    "                image_size=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH),\n",
    "                batch_size=self.config.BATCH_SIZE,\n",
    "                label_mode=\"categorical\",\n",
    "                shuffle=False,\n",
    "            )\n",
    "            .cache()\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        )\n",
    "        # Count files in datasets and update model.json\n",
    "        train_samples = (\n",
    "            tf.data.experimental.cardinality(self.train_dataset).numpy()\n",
    "            * self.config.BATCH_SIZE\n",
    "        )\n",
    "        val_samples = (\n",
    "            tf.data.experimental.cardinality(self.validation_dataset).numpy()\n",
    "            * self.config.BATCH_SIZE\n",
    "        )\n",
    "        test_samples = (\n",
    "            tf.data.experimental.cardinality(self.test_dataset).numpy()\n",
    "            * self.config.BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        dataset_info = {\n",
    "            \"dataset\": {\n",
    "                \"train_samples\": int(train_samples),\n",
    "                \"validation_samples\": int(val_samples),\n",
    "                \"test_samples\": int(test_samples),\n",
    "            }\n",
    "        }\n",
    "        self._update_model_json(\"data_pipeline_created\", dataset_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏁 Landscape Image Classifier - Part 3: Execution\n",
    "\n",
    "## 🚦 Pipeline Workflow\n",
    "\n",
    "1. **Initialization**\n",
    "   - GPU setup\n",
    "   - Directory creation\n",
    "   - Model building\n",
    "\n",
    "2. **Training Phase**\n",
    "   - Data loading & augmentation\n",
    "   - Model training with callbacks:\n",
    "     - Early stopping\n",
    "     - Learning rate reduction\n",
    "     - Model checkpointing\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Test accuracy\n",
    "   - Confusion matrix\n",
    "   - Classification report\n",
    "\n",
    "4. **Model Export**\n",
    "   - SavedModel (.pb)\n",
    "   - TFLite (.tflite)\n",
    "   - TensorFlow.js\n",
    "\n",
    "### Verification Checks:\n",
    "✅ GPU availability  \n",
    "✅ Model.json creation  \n",
    "✅ Training metrics  \n",
    "✅ File export validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected. Falling back to CPU.\n",
      "Initial model.json created at tfjs_model/model_info.json\n",
      "Initial TensorFlow.js model.json created at tfjs_model\\model.json\n",
      "Checking for model.json in tfjs_model\n",
      "✓ model.json exists in TFJS directory\n",
      "Building model...\n",
      "Updated model.json with status: model_built\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,542</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │         \u001b[38;5;34m1,542\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,841,385</span> (18.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,841,385\u001b[0m (18.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,317,142</span> (12.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,317,142\u001b[0m (12.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,524,243</span> (5.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,524,243\u001b[0m (5.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Updated model.json with status: data_pipeline_created\n",
      "Updated model.json with status: training_started\n",
      "Epoch 1/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.1833 - loss: 2.1096\n",
      "Epoch 1: val_accuracy improved from -inf to 0.17320, saving model to saved_models/best_model.keras\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 196ms/step - accuracy: 0.1833 - loss: 2.1095 - val_accuracy: 0.1732 - val_loss: 16.9961 - learning_rate: 1.0000e-04\n",
      "Epoch 2/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.2164 - loss: 1.9275\n",
      "Epoch 2: val_accuracy improved from 0.17320 to 0.17997, saving model to saved_models/best_model.keras\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 198ms/step - accuracy: 0.2164 - loss: 1.9274 - val_accuracy: 0.1800 - val_loss: 10.0672 - learning_rate: 1.0000e-04\n",
      "Epoch 3/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2255 - loss: 1.8604\n",
      "Epoch 3: val_accuracy did not improve from 0.17997\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 180ms/step - accuracy: 0.2255 - loss: 1.8604 - val_accuracy: 0.1746 - val_loss: 10.7070 - learning_rate: 1.0000e-04\n",
      "Epoch 4/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.2395 - loss: 1.8015\n",
      "Epoch 4: val_accuracy did not improve from 0.17997\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 177ms/step - accuracy: 0.2395 - loss: 1.8015 - val_accuracy: 0.1661 - val_loss: 14.1721 - learning_rate: 1.0000e-04\n",
      "Epoch 5/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2632 - loss: 1.7193\n",
      "Epoch 5: val_accuracy did not improve from 0.17997\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 178ms/step - accuracy: 0.2632 - loss: 1.7192 - val_accuracy: 0.1440 - val_loss: 11.1243 - learning_rate: 1.0000e-04\n",
      "Epoch 6/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2814 - loss: 1.6466\n",
      "Epoch 6: val_accuracy improved from 0.17997 to 0.18710, saving model to saved_models/best_model.keras\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 179ms/step - accuracy: 0.2814 - loss: 1.6466 - val_accuracy: 0.1871 - val_loss: 10.4830 - learning_rate: 1.0000e-04\n",
      "Epoch 7/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.2895 - loss: 1.6307\n",
      "Epoch 7: val_accuracy did not improve from 0.18710\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 183ms/step - accuracy: 0.2895 - loss: 1.6307 - val_accuracy: 0.1814 - val_loss: 6.7844 - learning_rate: 1.0000e-04\n",
      "Epoch 8/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3149 - loss: 1.5742\n",
      "Epoch 8: val_accuracy improved from 0.18710 to 0.18781, saving model to saved_models/best_model.keras\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 183ms/step - accuracy: 0.3149 - loss: 1.5743 - val_accuracy: 0.1878 - val_loss: 5.7719 - learning_rate: 1.0000e-04\n",
      "Epoch 9/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2986 - loss: 1.5679\n",
      "Epoch 9: val_accuracy improved from 0.18781 to 0.20456, saving model to saved_models/best_model.keras\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 181ms/step - accuracy: 0.2986 - loss: 1.5679 - val_accuracy: 0.2046 - val_loss: 6.4455 - learning_rate: 1.0000e-04\n",
      "Epoch 10/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.3207 - loss: 1.5332\n",
      "Epoch 10: val_accuracy did not improve from 0.20456\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 186ms/step - accuracy: 0.3208 - loss: 1.5332 - val_accuracy: 0.1978 - val_loss: 5.5298 - learning_rate: 1.0000e-04\n",
      "Epoch 11/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3288 - loss: 1.5289\n",
      "Epoch 11: val_accuracy improved from 0.20456 to 0.20634, saving model to saved_models/best_model.keras\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 184ms/step - accuracy: 0.3289 - loss: 1.5288 - val_accuracy: 0.2063 - val_loss: 7.6412 - learning_rate: 1.0000e-04\n",
      "Epoch 12/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3403 - loss: 1.4936\n",
      "Epoch 12: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 179ms/step - accuracy: 0.3403 - loss: 1.4936 - val_accuracy: 0.1900 - val_loss: 8.9230 - learning_rate: 1.0000e-04\n",
      "Epoch 13/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.3469 - loss: 1.4878\n",
      "Epoch 13: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 181ms/step - accuracy: 0.3469 - loss: 1.4878 - val_accuracy: 0.1946 - val_loss: 9.5117 - learning_rate: 1.0000e-04\n",
      "Epoch 14/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3595 - loss: 1.4570\n",
      "Epoch 14: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3975s\u001b[0m 6s/step - accuracy: 0.3595 - loss: 1.4570 - val_accuracy: 0.2017 - val_loss: 9.5709 - learning_rate: 1.0000e-04\n",
      "Epoch 15/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.3694 - loss: 1.4514\n",
      "Epoch 15: val_accuracy did not improve from 0.20634\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 195ms/step - accuracy: 0.3694 - loss: 1.4513 - val_accuracy: 0.1775 - val_loss: 14.7426 - learning_rate: 1.0000e-04\n",
      "Epoch 16/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.3674 - loss: 1.4252\n",
      "Epoch 16: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 195ms/step - accuracy: 0.3674 - loss: 1.4251 - val_accuracy: 0.1818 - val_loss: 18.5131 - learning_rate: 5.0000e-05\n",
      "Epoch 17/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.3941 - loss: 1.3864\n",
      "Epoch 17: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 183ms/step - accuracy: 0.3941 - loss: 1.3864 - val_accuracy: 0.1807 - val_loss: 14.4119 - learning_rate: 5.0000e-05\n",
      "Epoch 18/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.3905 - loss: 1.3873\n",
      "Epoch 18: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 184ms/step - accuracy: 0.3906 - loss: 1.3873 - val_accuracy: 0.1761 - val_loss: 22.7307 - learning_rate: 5.0000e-05\n",
      "Epoch 19/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.3954 - loss: 1.3794\n",
      "Epoch 19: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 186ms/step - accuracy: 0.3954 - loss: 1.3794 - val_accuracy: 0.1743 - val_loss: 21.2720 - learning_rate: 5.0000e-05\n",
      "Epoch 20/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.4023 - loss: 1.3653\n",
      "Epoch 20: val_accuracy did not improve from 0.20634\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 192ms/step - accuracy: 0.4023 - loss: 1.3653 - val_accuracy: 0.1750 - val_loss: 23.5957 - learning_rate: 5.0000e-05\n",
      "Epoch 21/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.4028 - loss: 1.3576\n",
      "Epoch 21: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 185ms/step - accuracy: 0.4029 - loss: 1.3575 - val_accuracy: 0.1753 - val_loss: 25.6958 - learning_rate: 2.5000e-05\n",
      "Epoch 22/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.4247 - loss: 1.3236\n",
      "Epoch 22: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 188ms/step - accuracy: 0.4247 - loss: 1.3236 - val_accuracy: 0.1828 - val_loss: 27.6699 - learning_rate: 2.5000e-05\n",
      "Epoch 23/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.4347 - loss: 1.3153\n",
      "Epoch 23: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 188ms/step - accuracy: 0.4347 - loss: 1.3153 - val_accuracy: 0.1675 - val_loss: 30.0300 - learning_rate: 2.5000e-05\n",
      "Epoch 24/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.4328 - loss: 1.3115\n",
      "Epoch 24: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 205ms/step - accuracy: 0.4328 - loss: 1.3115 - val_accuracy: 0.1728 - val_loss: 32.7777 - learning_rate: 2.5000e-05\n",
      "Epoch 25/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4403 - loss: 1.3162\n",
      "Epoch 25: val_accuracy did not improve from 0.20634\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 194ms/step - accuracy: 0.4403 - loss: 1.3162 - val_accuracy: 0.1768 - val_loss: 34.2417 - learning_rate: 2.5000e-05\n",
      "Epoch 26/35\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.4373 - loss: 1.3069\n",
      "Epoch 26: val_accuracy did not improve from 0.20634\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 190ms/step - accuracy: 0.4374 - loss: 1.3069 - val_accuracy: 0.1750 - val_loss: 34.2358 - learning_rate: 1.2500e-05\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Updated model.json with status: training_completed\n",
      "Critical error in main execution: 'LandscapeClassifier' object has no attribute 'plot_training_history'\n",
      "Emergency model.json and bin file created in tfjs_model directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    try:\n",
    "        configure_gpu(memory_limit=4096)\n",
    "        config = Config()\n",
    "        classifier = LandscapeClassifier(config)\n",
    "\n",
    "        # Check for model.json in TFJS directory\n",
    "        print(f\"Checking for model.json in {config.TFJS_MODEL_DIR}\")\n",
    "        if os.path.exists(os.path.join(config.TFJS_MODEL_DIR, \"model.json\")):\n",
    "            print(\"✓ model.json exists in TFJS directory\")\n",
    "        else:\n",
    "            print(\"✗ model.json not found in TFJS directory\")\n",
    "\n",
    "        # Build and display model summary\n",
    "        print(\"Building model...\")\n",
    "        classifier.build_model().summary()  # This should work if build_model is defined\n",
    "\n",
    "        # Start training\n",
    "        print(\"Starting training...\")\n",
    "        history = classifier.train()\n",
    "        classifier.plot_training_history(history)\n",
    "        classifier.evaluate()\n",
    "\n",
    "        # Save model in all required formats\n",
    "        print(\"\\n=== Saving model in multiple formats ===\")\n",
    "\n",
    "        # 1. SavedModel (.pb) format\n",
    "        print(\"\\nSaving model in SavedModel (.pb) format...\")\n",
    "        classifier.save_savedmodel()\n",
    "\n",
    "        # 2. TensorFlow Lite (.tflite) format\n",
    "        print(\"\\nSaving model in TFLite (.tflite) format...\")\n",
    "        classifier.save_tflite_model()\n",
    "\n",
    "        # 3. TensorFlow.js format\n",
    "        print(\"\\nSaving model in TensorFlow.js format...\")\n",
    "        classifier.save_tfjs_model()\n",
    "\n",
    "        # Final verification of required model files\n",
    "        print(\"\\n=== Final verification of required model files ===\")\n",
    "        savedmodel_dir = os.path.join(config.MODEL_SAVE_DIR, \"saved_model\")\n",
    "        if os.path.exists(\n",
    "            os.path.join(savedmodel_dir, \"saved_model.pb\")\n",
    "        ) and os.path.exists(os.path.join(savedmodel_dir, \"variables\")):\n",
    "            print(\"✅ SavedModel (.pb) format: COMPLETE\")\n",
    "        else:\n",
    "            print(\"❌ SavedModel (.pb) format: INCOMPLETE\")\n",
    "\n",
    "        if (\n",
    "            os.path.exists(config.TFLITE_PATH)\n",
    "            and os.path.getsize(config.TFLITE_PATH) > 0\n",
    "        ):\n",
    "            print(f\"✅ TensorFlow Lite (.tflite) format: COMPLETE\")\n",
    "        else:\n",
    "            print(f\"❌ TensorFlow Lite (.tflite) format: INCOMPLETE\")\n",
    "\n",
    "        tfjs_model_json = os.path.join(config.TFJS_MODEL_DIR, \"model.json\")\n",
    "        if os.path.exists(tfjs_model_json):\n",
    "            bin_files = [\n",
    "                f for f in os.listdir(config.TFJS_MODEL_DIR) if f.endswith(\".bin\")\n",
    "            ]\n",
    "            if bin_files:\n",
    "                print(\n",
    "                    f\"✅ TensorFlow.js format: COMPLETE (model.json and {len(bin_files)} bin files exist)\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"❌ TensorFlow.js format: INCOMPLETE (model.json exists but no bin files found)\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"❌ TensorFlow.js format: INCOMPLETE (model.json not found)\")\n",
    "\n",
    "        # Final update to model.json\n",
    "        with open(config.MODEL_JSON_PATH, \"r\") as f:\n",
    "            model_info = json.load(f)\n",
    "        model_info[\"pipeline_completed\"] = True\n",
    "        model_info[\"completion_time\"] = datetime.datetime.now().strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "        with open(config.MODEL_JSON_PATH, \"w\") as f:\n",
    "            json.dump(model_info, f, indent=4)\n",
    "\n",
    "        print(\n",
    "            f\"\\nProcessing complete! Check {config.MODEL_JSON_PATH} for full execution details.\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in main execution: {str(e)}\")\n",
    "        # Emergency fallback\n",
    "        emergency_info = {\n",
    "            \"status\": \"emergency_fallback\",\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "        os.makedirs(\"tfjs_model\", exist_ok=True)\n",
    "        with open(os.path.join(\"tfjs_model\", \"model.json\"), \"w\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    \"format\": \"tfjs-layers-model\",\n",
    "                    \"generatedBy\": \"emergency-fallback\",\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"weightsManifest\": [\n",
    "                        {\"paths\": [\"group1-shard1of1.bin\"], \"weights\": []}\n",
    "                    ],\n",
    "                },\n",
    "                f,\n",
    "                indent=2,\n",
    "            )\n",
    "        with open(os.path.join(\"tfjs_model\", \"group1-shard1of1.bin\"), \"wb\") as f:\n",
    "            f.write(b\"\\x00\" * 1024)\n",
    "        print(\"Emergency model.json and bin file created in tfjs_model directory\")\n",
    "        with open(\"model_info.json\", \"w\") as f:\n",
    "            json.dump(emergency_info, f, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
